{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Directory to be zipped\n",
    "# directory_path = '../data/anzu/anzu_LatestPublication'\n",
    "\n",
    "# # Path of the ZIP file to be created\n",
    "# zip_file_path = '../data/anzu/anzu_LatestPublication'\n",
    "\n",
    "# # Create the ZIP file\n",
    "# shutil.make_archive(zip_file_path, 'zip', directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Path of the ZIP file to be extracted\n",
    "zip_file_path = \"../data/anzu/anzu_LatestPublication.zip\"\n",
    "\n",
    "# Directory to extract the ZIP file to\n",
    "extract_dir = \"../data/anzu/anzu_LatestPublication\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get a list of all the Excel files in the directory\n",
    "excel_files = glob.glob(f\"{extract_dir}/*.xls\")\n",
    "\n",
    "# per file read the data for each tab and add it to a file dict\n",
    "data = {}\n",
    "for file in excel_files:\n",
    "    file_data = pd.read_excel(file, sheet_name=None, dtype=str)\n",
    "    data[file] = file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store the flattened data\n",
    "flattened_data = []\n",
    "\n",
    "# Iterate over the data dictionary\n",
    "for filename, tab_data in data.items():\n",
    "    # Iterate over each tab in the file\n",
    "    for tab_name, df in tab_data.items():\n",
    "        # Add the flattened data to the list\n",
    "        flattened_data.append([filename, tab_name, df])\n",
    "\n",
    "\n",
    "# Create the flat DataFrame\n",
    "flat_df = pd.DataFrame(flattened_data, columns=[\"Filename\", \"TabName\", \"DataFrame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df[\"df_length\"] = [len(df) for df in flat_df[\"DataFrame\"]]\n",
    "flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def txt_to_csv(file, col_nr=5):\n",
    "    new_filename = file.replace(\".txt\", \"_clean.csv\")\n",
    "\n",
    "    with open(file) as in_file:\n",
    "        reader = csv.reader(in_file, delimiter=\",\", quotechar='\"')\n",
    "        header_cols = len(next(reader))\n",
    "\n",
    "    with open(file) as in_file, open(new_filename, \"w\", newline=\"\") as out_file:\n",
    "        reader = csv.reader(in_file, delimiter=\",\", quotechar='\"')\n",
    "        writer = csv.writer(out_file, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for i, record in enumerate(reader):\n",
    "            cols = len(record)\n",
    "            if cols == header_cols:\n",
    "                writer.writerow(record)\n",
    "            elif cols > header_cols:\n",
    "                # Assuming extra commas are in the middle text fields only\n",
    "                new_line = record[:col_nr] + [\",\".join(record[col_nr : -(col_nr - 1)])] + record[-(col_nr - 1) :]\n",
    "                writer.writerow(new_line)\n",
    "            else:\n",
    "                print(f\"Error: Line {i} has fewer columns than expected. Ignoring lines.\")\n",
    "    return new_filename\n",
    "\n",
    "\n",
    "# read txt files as csv\n",
    "df_rows = []\n",
    "txt_files = glob.glob(f\"{extract_dir}/*.txt\")\n",
    "for file in txt_files:\n",
    "    try:\n",
    "        file_data = pd.read_csv(file, dtype=str)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Could not read {file}\")\n",
    "        clean_file = txt_to_csv(file)\n",
    "        file_data = pd.read_csv(clean_file, dtype=str)\n",
    "    file_name = file.split(\"/\")[-1].strip(\".txt\")\n",
    "    df_rows.append([file, file_name, file_data])\n",
    "\n",
    "txt_df = pd.DataFrame(df_rows, columns=[\"Filename\", \"TabName\", \"DataFrame\"])\n",
    "txt_df[\"df_length\"] = [len(df) for df in txt_df[\"DataFrame\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.concat([flat_df, txt_df])\n",
    "file_df[\"TabName\"] = file_df[\"TabName\"].str.replace(\"dbo.\", \"\")\n",
    "file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each TabName, check if all the columns are same and sort them by different columns list\n",
    "def sort_columns(df):\n",
    "    columns = df.columns\n",
    "    columns = sorted(columns)\n",
    "    return columns\n",
    "\n",
    "\n",
    "file_df[\"columns\"] = [sort_columns(df) for df in file_df[\"DataFrame\"]]\n",
    "file_df[\"columns_string\"] = file_df[\"TabName\"] + \"_\" + file_df[\"columns\"].astype(str)\n",
    "\n",
    "file_df[\"columns_string\"].value_counts()\n",
    "\n",
    "# check if multiple unique columns_string are present per TabName\n",
    "for tab in file_df[\"TabName\"].unique():\n",
    "    tab_df = file_df[file_df[\"TabName\"] == tab]\n",
    "    if len(tab_df[\"columns_string\"].unique()) > 1:\n",
    "        print(tab_df[\"columns_string\"].unique())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the tables with most number of rows per TabName\n",
    "file_df = file_df.sort_values(by=[\"df_length\"], ascending=False)\n",
    "file_df = file_df.drop_duplicates(subset=[\"TabName\"], keep=\"first\")\n",
    "file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each dataframe to a csv file with the TabName as the filename\n",
    "path = \"../data/anzu/anzu_cleaned\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for i, row in file_df.iterrows():\n",
    "    filename = row[\"TabName\"]\n",
    "    df = row[\"DataFrame\"]\n",
    "    df.to_csv(f\"{path}/{filename}.csv\", index=False)\n",
    "    print(f\"Written {filename}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aioc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
